{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **seq2seq 기계 번역**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1. 데이터 전처리**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import urllib3\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fra-eng.zip data download link : http://www.manythings.org/anki"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "zf = zipfile.ZipFile('fra-eng.zip')\n",
    "zf.extractall() \n",
    "zf.close()\n",
    "# with zipfile.ZipFile(zipfilename, 'r') as zip_ref:\n",
    "#     zip_ref.extractall(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "source sentence - target sentence - license 로 이뤄진 데이터셋에서 라이센스 열 지워주기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "190206"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines = pd.read_csv('fra.txt', names=['src', 'tar', 'lic'], sep='\\t')\n",
    "del lines['lic']\n",
    "len(lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "전체 샘플 수 190206개"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>src</th>\n",
       "      <th>tar</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>14608</td>\n",
       "      <td>It was unlocked.</td>\n",
       "      <td>Elle était déverrouillée.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26371</td>\n",
       "      <td>We were prisoners.</td>\n",
       "      <td>Nous étions prisonniers.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50422</td>\n",
       "      <td>Tell me what to think.</td>\n",
       "      <td>Dis-moi que penser !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14563</td>\n",
       "      <td>It looked funny.</td>\n",
       "      <td>Ça avait l'air drôle.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13880</td>\n",
       "      <td>I love your son.</td>\n",
       "      <td>Je suis amoureuse de ton fils.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10306</td>\n",
       "      <td>I love secrets.</td>\n",
       "      <td>J'adore les secrets.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33551</td>\n",
       "      <td>Did you do all this?</td>\n",
       "      <td>As-tu fait tout ceci ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19224</td>\n",
       "      <td>It's the big one.</td>\n",
       "      <td>C'est le gros.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46564</td>\n",
       "      <td>Do you know this word?</td>\n",
       "      <td>Connais-tu ce mot ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12144</td>\n",
       "      <td>We were joking.</td>\n",
       "      <td>Nous étions en train de plaisanter.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          src                                  tar\n",
       "14608        It was unlocked.            Elle était déverrouillée.\n",
       "26371      We were prisoners.             Nous étions prisonniers.\n",
       "50422  Tell me what to think.                 Dis-moi que penser !\n",
       "14563        It looked funny.                Ça avait l'air drôle.\n",
       "13880        I love your son.       Je suis amoureuse de ton fils.\n",
       "10306         I love secrets.                 J'adore les secrets.\n",
       "33551    Did you do all this?               As-tu fait tout ceci ?\n",
       "19224       It's the big one.                       C'est le gros.\n",
       "46564  Do you know this word?                  Connais-tu ce mot ?\n",
       "12144         We were joking.  Nous étions en train de plaisanter."
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines = lines.loc[:, 'src':'tar']   # 모든 행에 대해 'src', 'tar' 열 데이터만 가져와라\n",
    "lines = lines[0:60000]  #6만개의 샘플만 사용해 기계 번역기 구축\n",
    "lines.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "랜덤 선택된 10개의 샘플들  \n",
    "번역 문장에 해당하는 프랑스어 데이터엔 시작과 종료를 의미하는 sos, eos 심볼을 넣어주어야 함.  \n",
    "여기선 sos, eos 대신 '\\t', '\\n'을 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>src</th>\n",
       "      <th>tar</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>44433</td>\n",
       "      <td>Tom is such a coward.</td>\n",
       "      <td>\\t Tom n'est qu'un lâche. \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4554</td>\n",
       "      <td>Have a drink.</td>\n",
       "      <td>\\t Prenez une boisson ! \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33848</td>\n",
       "      <td>Everybody loves him.</td>\n",
       "      <td>\\t Tout le monde l'aime. \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40496</td>\n",
       "      <td>He was almost asleep.</td>\n",
       "      <td>\\t Il dormait presque. \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48358</td>\n",
       "      <td>I made a huge mistake.</td>\n",
       "      <td>\\t J'ai commis une énorme erreur. \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40370</td>\n",
       "      <td>He is reading a book.</td>\n",
       "      <td>\\t Il est en train de lire un livre. \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22401</td>\n",
       "      <td>Everyone is tired.</td>\n",
       "      <td>\\t Tout le monde est fatigué. \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44046</td>\n",
       "      <td>They're not all busy.</td>\n",
       "      <td>\\t Ils ne sont pas tous occupés. \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57440</td>\n",
       "      <td>The cat caught a mouse.</td>\n",
       "      <td>\\t Le chat a attrapé la souris. \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46461</td>\n",
       "      <td>Did you talk about me?</td>\n",
       "      <td>\\t As-tu parlé de moi ? \\n</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           src                                      tar\n",
       "44433    Tom is such a coward.             \\t Tom n'est qu'un lâche. \\n\n",
       "4554             Have a drink.               \\t Prenez une boisson ! \\n\n",
       "33848     Everybody loves him.              \\t Tout le monde l'aime. \\n\n",
       "40496    He was almost asleep.                \\t Il dormait presque. \\n\n",
       "48358   I made a huge mistake.     \\t J'ai commis une énorme erreur. \\n\n",
       "40370    He is reading a book.  \\t Il est en train de lire un livre. \\n\n",
       "22401       Everyone is tired.         \\t Tout le monde est fatigué. \\n\n",
       "44046    They're not all busy.      \\t Ils ne sont pas tous occupés. \\n\n",
       "57440  The cat caught a mouse.       \\t Le chat a attrapé la souris. \\n\n",
       "46461   Did you talk about me?               \\t As-tu parlé de moi ? \\n"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines.tar = lines.tar.apply(lambda x : '\\t ' + x + ' \\n')\n",
    "lines.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 글자 집합을 생성해야 함.  \n",
    "단어 집합이 아니라 글자 집합이라 하는 이유는 토큰 단위가 단어가 아니라 글자이기 때문."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_vocab = set()\n",
    "for line in lines.src:  # 1줄씩 read\n",
    "    for char in line:   # 1 글자씩 read\n",
    "        src_vocab.add(char)\n",
    "\n",
    "tar_vocab = set()\n",
    "for line in lines.tar:\n",
    "    for char in line:\n",
    "        tar_vocab.add(char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79\n",
      "105\n"
     ]
    }
   ],
   "source": [
    "src_vocab_size = len(src_vocab) + 1\n",
    "tar_vocab_size = len(tar_vocab) + 1\n",
    "print(src_vocab_size)\n",
    "print(tar_vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "영어와 프랑스어에는 각각 79, 105개의 글자가 존재.  \n",
    "이 중 인덱스를 임의로 부여해 일부만 출력해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
      "['T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w']\n"
     ]
    }
   ],
   "source": [
    "src_vocab = sorted(list(src_vocab))\n",
    "tar_vocab = sorted(list(tar_vocab))\n",
    "print(src_vocab[45:75])\n",
    "print(tar_vocab[45:75])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 각 글자에 인덱스를 부여해보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{' ': 1, '!': 2, '\"': 3, '$': 4, '%': 5, '&': 6, \"'\": 7, ',': 8, '-': 9, '.': 10, '/': 11, '0': 12, '1': 13, '2': 14, '3': 15, '4': 16, '5': 17, '6': 18, '7': 19, '8': 20, '9': 21, ':': 22, '?': 23, 'A': 24, 'B': 25, 'C': 26, 'D': 27, 'E': 28, 'F': 29, 'G': 30, 'H': 31, 'I': 32, 'J': 33, 'K': 34, 'L': 35, 'M': 36, 'N': 37, 'O': 38, 'P': 39, 'Q': 40, 'R': 41, 'S': 42, 'T': 43, 'U': 44, 'V': 45, 'W': 46, 'X': 47, 'Y': 48, 'Z': 49, 'a': 50, 'b': 51, 'c': 52, 'd': 53, 'e': 54, 'f': 55, 'g': 56, 'h': 57, 'i': 58, 'j': 59, 'k': 60, 'l': 61, 'm': 62, 'n': 63, 'o': 64, 'p': 65, 'q': 66, 'r': 67, 's': 68, 't': 69, 'u': 70, 'v': 71, 'w': 72, 'x': 73, 'y': 74, 'z': 75, 'é': 76, '’': 77, '€': 78}\n",
      "{'\\t': 1, '\\n': 2, ' ': 3, '!': 4, '\"': 5, '$': 6, '%': 7, '&': 8, \"'\": 9, '(': 10, ')': 11, ',': 12, '-': 13, '.': 14, '0': 15, '1': 16, '2': 17, '3': 18, '4': 19, '5': 20, '6': 21, '7': 22, '8': 23, '9': 24, ':': 25, '?': 26, 'A': 27, 'B': 28, 'C': 29, 'D': 30, 'E': 31, 'F': 32, 'G': 33, 'H': 34, 'I': 35, 'J': 36, 'K': 37, 'L': 38, 'M': 39, 'N': 40, 'O': 41, 'P': 42, 'Q': 43, 'R': 44, 'S': 45, 'T': 46, 'U': 47, 'V': 48, 'W': 49, 'X': 50, 'Y': 51, 'Z': 52, 'a': 53, 'b': 54, 'c': 55, 'd': 56, 'e': 57, 'f': 58, 'g': 59, 'h': 60, 'i': 61, 'j': 62, 'k': 63, 'l': 64, 'm': 65, 'n': 66, 'o': 67, 'p': 68, 'q': 69, 'r': 70, 's': 71, 't': 72, 'u': 73, 'v': 74, 'w': 75, 'x': 76, 'y': 77, 'z': 78, '\\xa0': 79, '«': 80, '»': 81, 'À': 82, 'Ç': 83, 'É': 84, 'Ê': 85, 'Ô': 86, 'à': 87, 'â': 88, 'ç': 89, 'è': 90, 'é': 91, 'ê': 92, 'ë': 93, 'î': 94, 'ï': 95, 'ô': 96, 'ù': 97, 'û': 98, 'œ': 99, '\\u2009': 100, '\\u200b': 101, '‘': 102, '’': 103, '\\u202f': 104}\n"
     ]
    }
   ],
   "source": [
    "src_to_index = dict([(word, i+1) for i, word in enumerate(src_vocab)])\n",
    "tar_to_index = dict([(word, i+1) for i, word in enumerate(tar_vocab)])\n",
    "print(src_to_index)\n",
    "print(tar_to_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 이를 가지고 훈련 데이터에 정수 인코딩을 수행해보자. 우선 인코더의 입력이 될 영어 문장 샘플에 대해서 정수 인코딩을 해보고, 5개의 샘플을 출력해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[30, 64, 10], [30, 64, 10], [30, 64, 10], [31, 58, 10], [31, 58, 10]]\n"
     ]
    }
   ],
   "source": [
    "encoder_input = []\n",
    "for line in lines.src:\n",
    "    temp_X = []\n",
    "    for w in line:\n",
    "        temp_X.append(src_to_index[w])\n",
    "    encoder_input.append(temp_X)\n",
    "print(encoder_input[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "디코더의 입력이 될 프랑스어 데이터에 대해서도 정수 인코딩을 해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 3, 48, 53, 3, 4, 3, 2], [1, 3, 39, 53, 70, 55, 60, 57, 14, 3, 2], [1, 3, 28, 67, 73, 59, 57, 3, 4, 3, 2], [1, 3, 45, 53, 64, 73, 72, 3, 4, 3, 2], [1, 3, 45, 53, 64, 73, 72, 14, 3, 2]]\n"
     ]
    }
   ],
   "source": [
    "decoder_input = []\n",
    "for line in lines.tar:\n",
    "    temp_X = []\n",
    "    for w in line:\n",
    "        temp_X.append(tar_to_index[w])\n",
    "    decoder_input.append(temp_X)\n",
    "print(decoder_input[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모든 decoder_input 원소의 앞에 1, 뒤엔 2가 붙어있다. 이를 통해 sos, eos 토큰이 제대로 붙은 것을 알 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아직 정수 인코딩을 수행해야 할 데이터가 하나 더 남아있다. 디코더의 예측값과 비교하기 위한 실제값이 필요하기 때문이다. 이 실제값에는 시작 심볼인 sos가 필요 없기 때문에 '\\t'를 제거해주어야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3, 48, 53, 3, 4, 3, 2], [3, 39, 53, 70, 55, 60, 57, 14, 3, 2], [3, 28, 67, 73, 59, 57, 3, 4, 3, 2], [3, 45, 53, 64, 73, 72, 3, 4, 3, 2], [3, 45, 53, 64, 73, 72, 14, 3, 2]]\n"
     ]
    }
   ],
   "source": [
    "decoder_target = []\n",
    "for line in lines.tar:\n",
    "    t=0\n",
    "    temp_X = []\n",
    "    for w in line:\n",
    "        if t>0:\n",
    "            temp_X.append(tar_to_index[w])\n",
    "        t=t+1\n",
    "    decoder_target.append(temp_X)\n",
    "print(decoder_target[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "패딩 작업을 수행해보자. 이를 위해 영어, 프랑스어 문장 각각에서 가장 긴 샘플의 길이를 알아보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24\n",
      "76\n"
     ]
    }
   ],
   "source": [
    "max_src_len = max([len(line) for line in lines.src])\n",
    "max_tar_len = max([len(line) for line in lines.tar])\n",
    "print(max_src_len)\n",
    "print(max_tar_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "영어 데이터는 영어 샘플들끼리, 프랑스어는 프랑스어 샘플들끼리 길이를 맞추어서 패딩  \n",
    "가장 긴 샘플의 길이에 맞춰서 영어 데이터의 샘플은 전부 길이가 24가 되도록 패딩하고, 프랑스어 데이터의 샘플은 전부 길이가 76이 되도록 패딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input = pad_sequences(encoder_input, maxlen=max_src_len, padding='post')\n",
    "decoder_input = pad_sequences(decoder_input, maxlen=max_tar_len, padding='post')\n",
    "decoder_target = pad_sequences(decoder_target, maxlen=max_tar_len, padding='post')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 모든 값에 대해서 원-핫 인코딩을 수행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input = to_categorical(encoder_input)\n",
    "decoder_input = to_categorical(decoder_input)\n",
    "decoder_target = to_categorical(decoder_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2. 모델 설계**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그런데 현재 시점의 디코더 셀의 입력은 오직 이전 디코더 셀의 출력을 입력으로 받는데 decoder_input이 왜 필요할까?\n",
    "\n",
    "훈련 과정에서는 이전 시점의 디코더 셀의 출력을 현재 시점의 디코더 셀의 입력으로 넣어주지 않고, 이전 시점의 실제값을 현재 시점의 디코더 셀의 입력값으로 하는 방법을 사용한다. 그 이유는 이전 시점의 디코더 셀의 예측이 틀렸는데 이를 현재 시점의 디코더 셀의 입력으로 사용하면 현재 시점의 디코더 셀의 예측도 잘못될 가능성이 높고 이는 연쇄 작용으로 디코더 전체의 예측을 어렵게 만든다. 이런 상황이 반복되면 훈련 시간이 느려진다. 만약 이 상황을 원하지 않는다면 이전 시점의 디코더 셀의 예측값 대신 실제값을 현재 시점의 디코더 셀의 입력으로 사용하는 방법을 사용할 수 있다. 이와 같이 RNN의 모든 시점에 대해서 이전 시점의 예측값 대신 <i><u>실제값</u></i>을 입력으로 주는 방법을 교사 강요(Teacher forcing)라고 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_inputs = Input(shape=(None, src_vocab_size))\n",
    "encoder_lstm = LSTM(units=256, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder_lstm(encoder_inputs)\n",
    "# encoder_outputs도 같이 리턴받기는 했지만 여기서는 필요없으므로 이 값은 버림.\n",
    "encoder_states = [state_h, state_c]\n",
    "# LSTM은 바닐라 RNN과는 달리 상태가 두 개. 바로 은닉 상태와 셀 상태."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "encoder_states는 디코더에 전달(컨텍스트 벡터)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_inputs = Input(shape=(None, tar_vocab_size))\n",
    "decoder_lstm = LSTM(units=256, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _= decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
    "# 디코더의 첫 상태를 인코더의 은닉 상태, 셀 상태로 합니다.\n",
    "decoder_softmax_layer = Dense(tar_vocab_size, activation='softmax')\n",
    "decoder_outputs = decoder_softmax_layer(decoder_outputs)\n",
    "\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "model.compile(optimizer=\"rmsprop\", loss=\"categorical_crossentropy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "디코더는 인코더의 마지막 은닉 상태를 초기 은닉 상태로 사용.  \n",
    "출력층에 프랑스어의 단어 집합의 크기만큼 뉴런을 배치한 후 소프트맥스 함수를 사용하여 실제값과의 오차를 구한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/40\n",
      "48000/48000 [==============================] - 23s 471us/sample - loss: 0.7348 - val_loss: 0.6605\n",
      "Epoch 2/40\n",
      "48000/48000 [==============================] - 15s 317us/sample - loss: 0.4538 - val_loss: 0.5307\n",
      "Epoch 3/40\n",
      "48000/48000 [==============================] - 15s 310us/sample - loss: 0.3828 - val_loss: 0.4723\n",
      "Epoch 4/40\n",
      "48000/48000 [==============================] - 14s 302us/sample - loss: 0.3414 - val_loss: 0.4395\n",
      "Epoch 5/40\n",
      "48000/48000 [==============================] - 15s 309us/sample - loss: 0.3130 - val_loss: 0.4088\n",
      "Epoch 6/40\n",
      "48000/48000 [==============================] - 15s 307us/sample - loss: 0.2919 - val_loss: 0.3925\n",
      "Epoch 7/40\n",
      "48000/48000 [==============================] - 15s 307us/sample - loss: 0.2758 - val_loss: 0.3820\n",
      "Epoch 8/40\n",
      "48000/48000 [==============================] - 15s 307us/sample - loss: 0.2628 - val_loss: 0.3713\n",
      "Epoch 9/40\n",
      "48000/48000 [==============================] - 15s 309us/sample - loss: 0.2518 - val_loss: 0.3653\n",
      "Epoch 10/40\n",
      "48000/48000 [==============================] - 15s 305us/sample - loss: 0.2425 - val_loss: 0.3602\n",
      "Epoch 11/40\n",
      "48000/48000 [==============================] - 15s 307us/sample - loss: 0.2344 - val_loss: 0.3565\n",
      "Epoch 12/40\n",
      "48000/48000 [==============================] - 15s 305us/sample - loss: 0.2271 - val_loss: 0.3532\n",
      "Epoch 13/40\n",
      "48000/48000 [==============================] - 15s 303us/sample - loss: 0.2206 - val_loss: 0.3516\n",
      "Epoch 14/40\n",
      "48000/48000 [==============================] - 15s 308us/sample - loss: 0.2146 - val_loss: 0.3506\n",
      "Epoch 15/40\n",
      "48000/48000 [==============================] - 15s 310us/sample - loss: 0.2091 - val_loss: 0.3504\n",
      "Epoch 16/40\n",
      "48000/48000 [==============================] - 15s 307us/sample - loss: 0.2040 - val_loss: 0.3494\n",
      "Epoch 17/40\n",
      "48000/48000 [==============================] - 15s 306us/sample - loss: 0.1993 - val_loss: 0.3511\n",
      "Epoch 18/40\n",
      "48000/48000 [==============================] - 15s 307us/sample - loss: 0.1949 - val_loss: 0.3505\n",
      "Epoch 19/40\n",
      "48000/48000 [==============================] - 15s 306us/sample - loss: 0.1907 - val_loss: 0.3520\n",
      "Epoch 20/40\n",
      "48000/48000 [==============================] - 15s 309us/sample - loss: 0.1869 - val_loss: 0.3530\n",
      "Epoch 21/40\n",
      "48000/48000 [==============================] - 15s 306us/sample - loss: 0.1831 - val_loss: 0.3543\n",
      "Epoch 22/40\n",
      "48000/48000 [==============================] - 15s 305us/sample - loss: 0.1797 - val_loss: 0.3545\n",
      "Epoch 23/40\n",
      "48000/48000 [==============================] - 15s 308us/sample - loss: 0.1764 - val_loss: 0.3567\n",
      "Epoch 24/40\n",
      "48000/48000 [==============================] - 15s 306us/sample - loss: 0.1732 - val_loss: 0.3573\n",
      "Epoch 25/40\n",
      "48000/48000 [==============================] - 15s 307us/sample - loss: 0.1702 - val_loss: 0.3605\n",
      "Epoch 26/40\n",
      "48000/48000 [==============================] - 15s 306us/sample - loss: 0.1674 - val_loss: 0.3632\n",
      "Epoch 27/40\n",
      "48000/48000 [==============================] - 15s 310us/sample - loss: 0.1647 - val_loss: 0.3648\n",
      "Epoch 28/40\n",
      "48000/48000 [==============================] - 15s 306us/sample - loss: 0.1621 - val_loss: 0.3673\n",
      "Epoch 29/40\n",
      "48000/48000 [==============================] - 15s 306us/sample - loss: 0.1595 - val_loss: 0.3672\n",
      "Epoch 30/40\n",
      "48000/48000 [==============================] - 15s 309us/sample - loss: 0.1572 - val_loss: 0.3706\n",
      "Epoch 31/40\n",
      "48000/48000 [==============================] - 15s 307us/sample - loss: 0.1549 - val_loss: 0.3739\n",
      "Epoch 32/40\n",
      "48000/48000 [==============================] - 15s 306us/sample - loss: 0.1528 - val_loss: 0.3749\n",
      "Epoch 33/40\n",
      "48000/48000 [==============================] - 15s 308us/sample - loss: 0.1505 - val_loss: 0.3780\n",
      "Epoch 34/40\n",
      "48000/48000 [==============================] - 15s 311us/sample - loss: 0.1485 - val_loss: 0.3810\n",
      "Epoch 35/40\n",
      "48000/48000 [==============================] - 15s 306us/sample - loss: 0.1467 - val_loss: 0.3834\n",
      "Epoch 36/40\n",
      "48000/48000 [==============================] - 15s 310us/sample - loss: 0.1447 - val_loss: 0.3855\n",
      "Epoch 37/40\n",
      "48000/48000 [==============================] - 15s 304us/sample - loss: 0.1428 - val_loss: 0.3890\n",
      "Epoch 38/40\n",
      "48000/48000 [==============================] - 15s 306us/sample - loss: 0.1412 - val_loss: 0.3897\n",
      "Epoch 39/40\n",
      "48000/48000 [==============================] - 15s 308us/sample - loss: 0.1395 - val_loss: 0.3928\n",
      "Epoch 40/40\n",
      "48000/48000 [==============================] - 15s 307us/sample - loss: 0.1379 - val_loss: 0.3952\n"
     ]
    }
   ],
   "source": [
    "with tf.device('/GPU:2'):\n",
    "    model.fit(x=[encoder_input, decoder_input], y=decoder_target, batch_size=64, epochs=40, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위에서 설정한 은닉 상태의 크기와 에포크 수는 실제로는 훈련 데이터에 과적합 상태를 불러온다.  \n",
    "중간부터 검증 데이터에 대한 오차인 val_loss의 값이 올라가는데, 사실 이번 실습에서는 주어진 데이터의 양과 태스크의 특성으로 인해 훈련 과정에서 훈련 데이터의 정확도와 과적합 방지라는 두 마리 토끼를 동시에 잡기에는 쉽지 않다.  \n",
    "여기서는 우선 seq2seq의 메커니즘과 짧은 문장과 긴 문장에 대한 성능 차이에 대한 확인을 중점으로 두고 훈련 데이터에 과적합 된 상태로 동작 단계로 넘어간다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3. seq2seq 기계 번역기 동작시키기**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "전체적인 번역 동작 단계를 정리하면 아래와 같습니다.\n",
    "1. 번역하고자 하는 입력 문장이 인코더에 들어가서 은닉 상태와 셀 상태를 얻습니다.\n",
    "2. 상태와 SOS에 해당하는 '\\t'를 디코더로 보냅니다.\n",
    "3. 디코더가 EOS에 해당하는 '\\n'이 나올 때까지 다음 문자를 예측하는 행동을 반복합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "인코더 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_model = Model(inputs=encoder_inputs, outputs=encoder_states)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "디코더 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이전 시점의 상태들을 저장하는 텐서\n",
    "decoder_state_input_h = Input(shape=(256,))\n",
    "decoder_state_input_c = Input(shape=(256,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(decoder_inputs, initial_state=decoder_states_inputs)\n",
    "# 문장의 다음 단어를 예측하기 위해서 초기 상태(initial_state)를 이전 시점의 상태로 사용. 이는 뒤의 함수 decode_sequence()에 구현\n",
    "decoder_states = [state_h, state_c]\n",
    "# 훈련 과정에서와 달리 LSTM의 리턴하는 은닉 상태와 셀 상태인 state_h와 state_c를 버리지 않음.\n",
    "decoder_outputs = decoder_softmax_layer(decoder_outputs)\n",
    "decoder_model = Model(inputs=[decoder_inputs] + decoder_states_inputs, outputs=[decoder_outputs] + decoder_states)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "인덱스로부터 단어를 만드는 index_to_src, index_to_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_to_src = dict((i, char) for char, i in src_to_index.items())\n",
    "index_to_tar = dict((i, char) for char, i in tar_to_index.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    # 입력으로부터 인코더의 상태를 얻음\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    # <SOS>에 해당하는 원-핫 벡터 생성\n",
    "    target_seq = np.zeros((1, 1, tar_vocab_size))\n",
    "    target_seq[0, 0, tar_to_index['\\t']] = 1.\n",
    "\n",
    "    stop_condition = False\n",
    "    decoded_sentence = \"\"\n",
    "\n",
    "    # stop_condition이 True가 될 때까지 루프 반복\n",
    "    while not stop_condition:\n",
    "        # 이점 시점의 상태 states_value를 현 시점의 초기 상태로 사용\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
    "\n",
    "        # 예측 결과를 문자로 변환\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = index_to_tar[sampled_token_index]\n",
    "\n",
    "        # 현재 시점의 예측 문자를 예측 문장에 추가\n",
    "        decoded_sentence += sampled_char\n",
    "\n",
    "        # <eos>에 도달하거나 최대 길이를 넘으면 중단.\n",
    "        if (sampled_char == '\\n' or len(decoded_sentence) > max_tar_len):\n",
    "            stop_condition = True\n",
    "\n",
    "        # 현재 시점의 예측 결과를 다음 시점의 입력으로 사용하기 위해 저장\n",
    "        target_seq = np.zeros((1, 1, tar_vocab_size))\n",
    "        target_seq[0, 0, sampled_token_index] = 1.\n",
    "\n",
    "        # 현재 시점의 상태를 다음 시점의 상태로 사용하기 위해 저장\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------\n",
      "입력 문장: Hi.\n",
      "정답 문장:  Salut ! \n",
      "번역기가 번역한 문장:  Salut. \n",
      "-----------------------------------\n",
      "입력 문장: I see.\n",
      "정답 문장:  Aha. \n",
      "번역기가 번역한 문장:  Je compte. \n",
      "-----------------------------------\n",
      "입력 문장: Hug me.\n",
      "정답 문장:  Serrez-moi dans vos bras ! \n",
      "번역기가 번역한 문장:  Serrez-moi comme un coup ! \n",
      "-----------------------------------\n",
      "입력 문장: Hold it!\n",
      "정답 문장:  Restez où vous êtes ! \n",
      "번역기가 번역한 문장:  Ne bougez plus ! \n",
      "-----------------------------------\n",
      "입력 문장: I crashed.\n",
      "정답 문장:  Je me suis écrasée. \n",
      "번역기가 번역한 문장:  Je suis détendu. \n"
     ]
    }
   ],
   "source": [
    "for seq_index in [3,50,100,300,1001]: # 입력 문장의 인덱스\n",
    "    input_seq = encoder_input[seq_index: seq_index + 1]\n",
    "    decoded_sentence = decode_sequence(input_seq)\n",
    "    print(35 * \"-\")\n",
    "    print('입력 문장:', lines.src[seq_index])\n",
    "    print('정답 문장:', lines.tar[seq_index][1:len(lines.tar[seq_index])-1]) # '\\t'와 '\\n'을 빼고 출력\n",
    "    print('번역기가 번역한 문장:', decoded_sentence[:len(decoded_sentence)-1]) # '\\n'을 빼고 출력"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "30274a649c5cdc7c99b850337d57129588e9992e587b6fd9d381ea981b7a9f30"
  },
  "kernelspec": {
   "display_name": "Python 3.6.13 64-bit ('nlp-tutorial': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
